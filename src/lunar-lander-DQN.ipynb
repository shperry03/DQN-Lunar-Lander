{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# DQN in OpenAI Lunar Lander"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Import OpenAI gym and load lunar lander game as environment"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Python310\\lib\\site-packages\\gym\\envs\\registration.py:563: UserWarning: \u001b[33mWARN: Using the latest versioned environment `LunarLander-v2` instead of the unversioned environment `LunarLander`.\u001b[0m\n","  logger.warn(\n"]}],"source":["import gym as gym\n","env = gym.make(\"LunarLander\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Import all the necessary pytorch libraries and others"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"data":{"text/plain":["<contextlib.ExitStack at 0x19b4becff10>"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["import math\n","import random\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from collections import namedtuple, deque\n","from itertools import count\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as fun\n","\n","\n","is_ipython = 'inline' in matplotlib.get_backend()\n","if is_ipython:\n","    from IPython import display\n","\n","plt.ion()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Set up the memory caching and sampling for the training portion of our model."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["Transition = namedtuple('Transition','state action next_state reward')\n","\n","class Memory(object):\n","\n","    def __init__(self) -> None:\n","        self.memory = deque(maxlen=10000)\n","        self.batch_size = 128\n","    \n","    def cache(self, *args):\n","        self.memory.append(Transition(*args))\n","    \n","    def recall(self):\n","        return random.sample(self.memory,self.batch_size)\n","    \n","    def __len__(self):\n","        return len(self.memory)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Get the number of actions and observations from the Lunar Landing environment."]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["num_actions = env.action_space.n\n","\n","state,info = env.reset()\n","num_observations = len(state)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["class DQN(nn.Module):\n","    \n","    def __init__(self,num_observations,num_actions):\n","        super(DQN,self).__init__()\n","        self.layer1 = nn.Linear(num_observations, 128)\n","        self.layer2 = nn.Linear(128,128)\n","        self.layer3 = nn.Linear(128,128)\n","        self.layer4 = nn.Linear(128,num_actions)\n","\n","    def forward(self,x):\n","        x = fun.leaky_relu(self.layer1(x))\n","        x = fun.leaky_relu(self.layer2(x))\n","        x = fun.leaky_relu(self.layer3(x))\n","        return self.layer3(x)\n","\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["BATCH_SIZE = 128\n","GAMMA = 0.99\n","EPS_START = 0.9\n","EPS_END = 0.05\n","EPS_DECAY = 1000\n","TAU = 0.001\n","LR = 1e-4\n","\n","policy_net = DQN(num_observations, num_actions)\n","target_net = DQN(num_observations, num_actions)\n","target_net.load_state_dict(policy_net.state_dict())\n","\n","optimizer = optim.AdamW(policy_net.parameters(),lr=LR,amsgrad=True)\n","memory = Memory()"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["steps = 0 \n","\n","def act(state):\n","    global steps\n","    sample = random.random()\n","    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n","        math.exp(-1. * steps / EPS_DECAY)\n","    steps += 1\n","    if sample > eps_threshold:\n","        with torch.no_grad():\n","            return policy_net(state).max(1)[1].view(1, 1)\n","    else:\n","        return torch.tensor([[env.action_space.sample()]], dtype=torch.long)"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["episode_durations = []\n","\n","\n","def plot_durations(show_result=False):\n","    plt.figure(1)\n","    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n","    if show_result:\n","        plt.title('Reward per Episode')\n","    else:\n","        plt.clf()\n","        plt.title('Training...')\n","    plt.xlabel('Episode')\n","    plt.ylabel('Reward')\n","    plt.plot(durations_t.numpy())\n","    \n","    if len(durations_t) >= 100:\n","        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n","        means = torch.cat((torch.zeros(99), means))\n","        plt.plot(means.numpy())\n","\n","    plt.pause(0.001)  \n","    if is_ipython:\n","        if not show_result:\n","            display.display(plt.gcf())\n","            display.clear_output(wait=True)\n","        else:\n","            display.display(plt.gcf())"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["def optimize_model():\n","    if len(memory) < BATCH_SIZE:\n","        return\n","    transitions = memory.recall()\n","\n","    batch = Transition(*zip(*transitions))\n","\n","    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n","                                          batch.next_state)), dtype=torch.bool)\n","    non_final_next_states = torch.cat([s for s in batch.next_state\n","                                                if s is not None])\n","    state_batch = torch.cat(batch.state)\n","    action_batch = torch.cat(batch.action)\n","    reward_batch = torch.cat(batch.reward)\n","\n","    state_action_values = policy_net(state_batch).gather(1, action_batch)\n","\n","    next_state_values = torch.zeros(BATCH_SIZE)\n","    with torch.no_grad():\n","        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n","    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n","\n","    criterion = nn.SmoothL1Loss()\n","    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n","\n","    optimizer.zero_grad()\n","    loss.backward()\n","\n","    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n","    optimizer.step()"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"ename":"AssertionError","evalue":"95 (<class 'int'>) invalid ","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[1;32mc:\\Users\\sperr\\OneDrive\\Desktop\\Classes\\CS Courses\\CS 591-Gong\\final_project\\lunar-lander-DQN.ipynb Cell 15\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sperr/OneDrive/Desktop/Classes/CS%20Courses/CS%20591-Gong/final_project/lunar-lander-DQN.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m count():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/sperr/OneDrive/Desktop/Classes/CS%20Courses/CS%20591-Gong/final_project/lunar-lander-DQN.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     action \u001b[39m=\u001b[39m act(state)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/sperr/OneDrive/Desktop/Classes/CS%20Courses/CS%20591-Gong/final_project/lunar-lander-DQN.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     observation, reward, terminated, truncated, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action\u001b[39m.\u001b[39;49mitem())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sperr/OneDrive/Desktop/Classes/CS%20Courses/CS%20591-Gong/final_project/lunar-lander-DQN.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     reward_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/sperr/OneDrive/Desktop/Classes/CS%20Courses/CS%20591-Gong/final_project/lunar-lander-DQN.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([reward])\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\gym\\wrappers\\time_limit.py:50\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     40\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[0;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     53\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\gym\\wrappers\\env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     38\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n","File \u001b[1;32mc:\\Python310\\lib\\site-packages\\gym\\envs\\box2d\\lunar_lander.py:482\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    480\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(action, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m    481\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 482\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(\n\u001b[0;32m    483\u001b[0m         action\n\u001b[0;32m    484\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00maction\u001b[39m!r}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(action)\u001b[39m}\u001b[39;00m\u001b[39m) invalid \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    486\u001b[0m \u001b[39m# Engines\u001b[39;00m\n\u001b[0;32m    487\u001b[0m tip \u001b[39m=\u001b[39m (math\u001b[39m.\u001b[39msin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mangle), math\u001b[39m.\u001b[39mcos(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlander\u001b[39m.\u001b[39mangle))\n","\u001b[1;31mAssertionError\u001b[0m: 95 (<class 'int'>) invalid "]}],"source":["EPOCHS = 200\n","\n","for i_episode in range(EPOCHS):\n","    reward_total = 0\n","    \n","    state, info = env.reset()\n","    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n","    for t in count():\n","        action = act(state)\n","        observation, reward, terminated, truncated, _ = env.step(action.item())\n","        reward_total += reward\n","        reward = torch.tensor([reward])\n","        done = terminated or truncated\n","\n","        if terminated:\n","            print(\"REWARD = \",reward)\n","            next_state = None\n","        else:\n","            next_state = torch.tensor(observation, dtype=torch.float32).unsqueeze(0)\n","\n","        # Store the transition in memory\n","        memory.cache(state, action, next_state, reward)\n","\n","        # Move to the next state\n","        state = next_state\n","\n","        # Perform one step of the optimization (on the policy network)\n","        optimize_model()\n","\n","        # Soft update of the target network's weights\n","        # θ′ ← τ θ + (1 −τ )θ′\n","        target_net_state_dict = target_net.state_dict()\n","        policy_net_state_dict = policy_net.state_dict()\n","        for key in policy_net_state_dict:\n","            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n","        target_net.load_state_dict(target_net_state_dict)\n","\n","        if done:\n","            episode_durations.append(reward_total)\n","            plot_durations()\n","            break\n","\n","plot_durations(show_result=True)\n","plt.ioff()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"}}},"nbformat":4,"nbformat_minor":2}
